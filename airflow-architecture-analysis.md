# Airflow ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­

## ğŸ“‹ ê°œìš”

í˜„ì¬ ë©€í‹°íƒœìŠ¤í¬ Airflow ì•„í‚¤í…ì²˜ì˜ ë°ì´í„° ë¬´ê²°ì„± ë¬¸ì œë¥¼ ë¶„ì„í•˜ê³ , ë‹¨ì¼ í†µí•© íƒœìŠ¤í¬ ì•„í‚¤í…ì²˜ë¡œì˜ ì „í™˜ì„ í†µí•œ í•´ê²°ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.

## ğŸš¨ í˜„ì¬ ì•„í‚¤í…ì²˜ì˜ ë¬¸ì œì 

### 1. ë°ì´í„° ë¬´ê²°ì„± ìœ„í—˜
- **ì—°ê²° ID ë¶ˆì¼ì¹˜**: íƒœìŠ¤í¬ë³„ë¡œ ë‹¤ë¥¸ connection ID ì‚¬ìš©
- **ì‹œê°„ ë™ê¸°í™” ë¬¸ì œ**: ê° íƒœìŠ¤í¬ê°€ ì„œë¡œ ë‹¤ë¥¸ ì‹œê°„ ê¸°ì¤€ì  ì‚¬ìš©
- **ë°ì´í„° ê°­/ì¤‘ë³µ**: ì‹œê°„ ìœˆë„ìš° ë¶ˆì¼ì¹˜ë¡œ ì¸í•œ ëˆ„ë½ ë° ì¤‘ë³µ ì²˜ë¦¬ ìœ„í—˜

### 2. ë³µì¡í•œ ì¡°ì • ë©”ì»¤ë‹ˆì¦˜
- **XCom ì˜ì¡´ì„±**: íƒœìŠ¤í¬ ê°„ ë°ì´í„° ì „ë‹¬ì„ ìœ„í•œ ë³µì¡í•œ ì¡°ì •
- **ë‹¤ì¤‘ ì‹¤íŒ¨ ì§€ì **: ì—¬ëŸ¬ íƒœìŠ¤í¬ì˜ ë…ë¦½ì  ì‹¤íŒ¨ ê°€ëŠ¥ì„±
- **ë³µêµ¬ ë³µì¡ì„±**: ë¶€ë¶„ ì‹¤íŒ¨ ì‹œ ë³µêµ¬ ê³¼ì •ì˜ ë³µì¡ì„±

## âœ… ê¶Œì¥ í•´ê²°ë°©ì•ˆ: ë‹¨ì¼ í†µí•© ì•„í‚¤í…ì²˜

### í•µì‹¬ ì¥ì 

#### ğŸ›¡ï¸ ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥
- **ì›ìì  ì—°ì‚°**: ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ ì„±ê³µí•˜ê±°ë‚˜ ì‹¤íŒ¨ (all-or-nothing)
- **ì¼ê´€ëœ ì‹œê°„ ì»¨í…ìŠ¤íŠ¸**: ë‹¨ì¼ `data_interval_start/end` ì‚¬ìš©
- **ë‹¨ì¼ ì—°ê²°**: í•˜ë‚˜ì˜ `postgres_default` ì—°ê²°ë¡œ ì¼ê´€ì„± í™•ë³´

#### ğŸ”§ ìš´ì˜ ë‹¨ìˆœí™”
- **XCom ì œê±°**: í•¨ìˆ˜ ë‚´ ì§ì ‘ ë³€ìˆ˜ ì „ë‹¬
- **ë‹¨ì¼ ì‹¤íŒ¨ ì§€ì **: ê°„ë‹¨í•œ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë¡¤ë°±
- **ëª¨ë‹ˆí„°ë§ ê°„ì†Œí™”**: í•˜ë‚˜ì˜ íƒœìŠ¤í¬ ìƒíƒœ ì¶”ì 

#### ğŸ“ˆ Airflow ëª¨ë²” ì‚¬ë¡€ ì¤€ìˆ˜
- **íŠ¸ëœì­ì…˜ ì›ì¹™**: "ë°ì´í„°ë² ì´ìŠ¤ íŠ¸ëœì­ì…˜"ìœ¼ë¡œ íƒœìŠ¤í¬ ì²˜ë¦¬
- **ë©±ë“±ì„±**: UPSERT ì—°ì‚°ìœ¼ë¡œ ì¬ì‹¤í–‰ ì•ˆì „ì„± ë³´ì¥
- **ì¼ê´€ëœ ê²°ê³¼**: ë§¤ë²ˆ ë™ì¼í•œ ê²°ê³¼ ë³´ì¥

## ğŸ• ì‹œê°„ëŒ€ ì²˜ë¦¬ ì „ëµ (Apache Airflow ê³µì‹ ê°€ì´ë“œ)

### í•µì‹¬ ì›ì¹™
- **Airflow ë‚´ë¶€**: ëª¨ë“  datetimeì„ UTCë¡œ ì €ì¥ ë° ì²˜ë¦¬
- **ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§**: ì‹¤ì œ DB ì‹œê°„ëŒ€ì— ë§ì¶° ë³€í™˜ í›„ ì²˜ë¦¬
- **ê³µì‹ ê¶Œì¥**: Pendulum ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ìœ¼ë¡œ ì •í™•í•œ ì‹œê°„ëŒ€ ë³€í™˜

### ì‹œê°„ëŒ€ ë³€í™˜ êµ¬í˜„ íŒ¨í„´

```python
import pendulum
from airflow.sdk import task
from airflow.providers.postgres.hooks.postgres import PostgresHook

@task
def unified_data_sync_with_timezone(context):
    """ì‹œê°„ëŒ€ ë³€í™˜ì„ í¬í•¨í•œ í†µí•© ë°ì´í„° ë™ê¸°í™”"""
    
    # 1. Airflow UTC ì‹œê°„ ë°›ê¸° (ê³µì‹ ê¶Œì¥)
    start_time_utc = context['data_interval_start']
    end_time_utc = context['data_interval_end']
    
    # 2. í•œêµ­ ì‹œê°„ëŒ€ë¡œ ë³€í™˜ (DBê°€ KSTì¸ ê²½ìš°)
    korea_tz = pendulum.timezone("Asia/Seoul")
    start_time_kst = korea_tz.convert(start_time_utc)
    end_time_kst = korea_tz.convert(end_time_utc)
    
    # 3. ì‹œê°„ëŒ€ ë³€í™˜ ë¡œê¹… (ê²€ì¦ìš©)
    logger.info(f"ì‹œê°„ ìœˆë„ìš° ë³€í™˜:")
    logger.info(f"  UTC: {start_time_utc} ~ {end_time_utc}")
    logger.info(f"  KST: {start_time_kst} ~ {end_time_kst}")
    logger.info(f"  ì‹œì°¨: +{start_time_kst.offset_hours}ì‹œê°„")
    
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    try:
        with hook.get_conn() as conn:
            with conn.cursor() as cur:
                # 4. KST ì‹œê°„ìœ¼ë¡œ DB ì¡°íšŒ (DBê°€ í•œêµ­ ì‹œê°„ëŒ€ì´ë¯€ë¡œ)
                extract_query = """
                    SELECT * FROM source_table 
                    WHERE created_at >= %s AND created_at < %s
                """
                cur.execute(extract_query, (start_time_kst, end_time_kst))
                raw_data = cur.fetchall()
                
                logger.info(f"ì¡°íšŒëœ ë ˆì½”ë“œ ìˆ˜: {len(raw_data)}")
                
                # 5. ë°ì´í„° ë³€í™˜ ë° ì²˜ë¦¬
                transformed_data = transform_data(raw_data)
                
                # 6. UPSERT (ë©±ë“±ì„± ë³´ì¥)
                upsert_query = """
                    INSERT INTO target_table (id, data, updated_at)
                    VALUES %s
                    ON CONFLICT (id) 
                    DO UPDATE SET 
                        data = EXCLUDED.data,
                        updated_at = EXCLUDED.updated_at
                """
                execute_values(cur, upsert_query, transformed_data)
                
        return {
            "status": "success",
            "processed_records": len(transformed_data),
            "utc_window": f"{start_time_utc} ~ {end_time_utc}",
            "kst_window": f"{start_time_kst} ~ {end_time_kst}",
            "timezone_offset": f"+{start_time_kst.offset_hours}h"
        }
        
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        raise

def transform_data(raw_data):
    """ë°ì´í„° ë³€í™˜ ë¡œì§"""
    # ë³€í™˜ ë¡œì§ êµ¬í˜„
    return transformed_data
```

### ì‹œê°„ëŒ€ ì²˜ë¦¬ ê²€ì¦ ë° ì•ˆì „ì¥ì¹˜

```python
def validate_timezone_conversion(utc_time, local_time, expected_offset_hours=9):
    """ì‹œê°„ëŒ€ ë³€í™˜ ì •í™•ì„± ê²€ì¦"""
    actual_offset = (local_time - utc_time).total_seconds() / 3600
    
    if abs(actual_offset - expected_offset_hours) > 0.1:  # 6ë¶„ í—ˆìš© ì˜¤ì°¨
        raise ValueError(
            f"ì‹œê°„ëŒ€ ë³€í™˜ ì˜¤ë¥˜: ì˜ˆìƒ {expected_offset_hours}h, ì‹¤ì œ {actual_offset}h"
        )
    
    logger.info(f"ì‹œê°„ëŒ€ ë³€í™˜ ê²€ì¦ ì„±ê³µ: {actual_offset}ì‹œê°„ ì°¨ì´")
    return True

@task
def unified_data_sync_with_validation(context):
    """ì‹œê°„ëŒ€ ê²€ì¦ì„ í¬í•¨í•œ ì•ˆì „í•œ ë°ì´í„° ë™ê¸°í™”"""
    start_time_utc = context['data_interval_start']
    end_time_utc = context['data_interval_end']
    
    korea_tz = pendulum.timezone("Asia/Seoul")
    start_time_kst = korea_tz.convert(start_time_utc)
    end_time_kst = korea_tz.convert(end_time_utc)
    
    # ì‹œê°„ëŒ€ ë³€í™˜ ê²€ì¦
    validate_timezone_conversion(start_time_utc, start_time_kst)
    
    # DST(ì„œë¨¸íƒ€ì„) ê³ ë ¤ ê²€ì¦
    if start_time_kst.dst().total_seconds() != 0:
        logger.warning(f"DST ì ìš© ê¸°ê°„: {start_time_kst.dst()}")
    
    # ì´í›„ ê¸°ì¡´ ë¡œì§ ì‹¤í–‰...
```

## ğŸš€ êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: ì¦‰ì‹œ êµ¬í˜„ (1-2ì¼)
**ëª©í‘œ**: ì‹œê°„ëŒ€ ë³€í™˜ì„ í¬í•¨í•œ ê¸°ë³¸ ë‹¨ì¼ íƒœìŠ¤í¬ ì•„í‚¤í…ì²˜ êµ¬í˜„

```python
@task
def unified_data_sync(context):
    """í†µí•© ë°ì´í„° ë™ê¸°í™” íƒœìŠ¤í¬ (ì‹œê°„ëŒ€ ë³€í™˜ í¬í•¨)"""
    # ë‹¨ì¼ ì‹œê°„ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© (UTC â†’ KST ë³€í™˜)
    start_time_utc = context['data_interval_start']
    end_time_utc = context['data_interval_end']
    
    # í•œêµ­ ì‹œê°„ëŒ€ë¡œ ë³€í™˜ (DBê°€ KSTì¸ ê²½ìš°)
    korea_tz = pendulum.timezone("Asia/Seoul")
    start_time_kst = korea_tz.convert(start_time_utc)
    end_time_kst = korea_tz.convert(end_time_utc)
    
    # ë‹¨ì¼ ì—°ê²° ì‚¬ìš©
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    try:
        with hook.get_conn() as conn:
            with conn.cursor() as cur:
                # ëª¨ë“  ë°ì´í„° ì—°ì‚°ì„ ë‹¨ì¼ íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì²˜ë¦¬
                
                # Step 1: ë°ì´í„° ì¶”ì¶œ (KST ê¸°ì¤€ìœ¼ë¡œ ì¡°íšŒ)
                extract_query = """
                    SELECT * FROM source_table 
                    WHERE created_at >= %s AND created_at < %s
                """
                cur.execute(extract_query, (start_time_kst, end_time_kst))
                raw_data = cur.fetchall()
                
                # Step 2: ë°ì´í„° ë³€í™˜
                transformed_data = transform_data(raw_data)
                
                # Step 3: ë°ì´í„° ì ì¬ (UPSERT for idempotency)
                upsert_query = """
                    INSERT INTO target_table (id, data, updated_at)
                    VALUES %s
                    ON CONFLICT (id) 
                    DO UPDATE SET 
                        data = EXCLUDED.data,
                        updated_at = EXCLUDED.updated_at
                """
                execute_values(cur, upsert_query, transformed_data)
                
                # íŠ¸ëœì­ì…˜ ì»¤ë°‹ì€ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨
                
        return {
            "status": "success", 
            "processed_records": len(transformed_data),
            "utc_window": f"{start_time_utc} - {end_time_utc}",
            "kst_window": f"{start_time_kst} - {end_time_kst}"
        }
        
    except Exception as e:
        # ë‹¨ì¼ ë¡¤ë°± ì§€ì 
        logger.error(f"Pipeline failed: {e}")
        raise

def transform_data(raw_data):
    """ë°ì´í„° ë³€í™˜ ë¡œì§"""
    # ë³€í™˜ ë¡œì§ êµ¬í˜„
    return transformed_data
```

### Phase 2: ë‹¨ê¸° ê°œì„  (1ì£¼)
**ëª©í‘œ**: ì•ˆì •ì„± ë° ëª¨ë‹ˆí„°ë§ ê°•í™”

#### 2-1. ë©±ë“±ì„± ê°•í™”
```python
@task
def unified_data_sync_enhanced(context):
    """í–¥ìƒëœ í†µí•© ë°ì´í„° ë™ê¸°í™”"""
    start_time = context['data_interval_start']
    end_time = context['data_interval_end']
    
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€ë¥¼ ìœ„í•œ ì²´í¬
    processing_id = f"{context['dag_run'].run_id}_{start_time.isoformat()}"
    
    try:
        with hook.get_conn() as conn:
            with conn.cursor() as cur:
                # ì´ë¯¸ ì²˜ë¦¬ëœ ë°°ì¹˜ì¸ì§€ í™•ì¸
                cur.execute(
                    "SELECT 1 FROM processing_log WHERE processing_id = %s",
                    (processing_id,)
                )
                if cur.fetchone():
                    logger.info(f"Batch {processing_id} already processed, skipping")
                    return {"status": "skipped", "reason": "already_processed"}
                
                # ì²˜ë¦¬ ì‹œì‘ ë¡œê·¸
                cur.execute(
                    "INSERT INTO processing_log (processing_id, status, start_time) VALUES (%s, 'running', %s)",
                    (processing_id, datetime.now())
                )
                
                # ë°ì´í„° ì²˜ë¦¬ ë¡œì§
                processed_count = process_data_batch(cur, start_time, end_time)
                
                # ì²˜ë¦¬ ì™„ë£Œ ë¡œê·¸
                cur.execute(
                    "UPDATE processing_log SET status = 'completed', end_time = %s, processed_count = %s WHERE processing_id = %s",
                    (datetime.now(), processed_count, processing_id)
                )
                
        return {"status": "success", "processed_records": processed_count}
        
    except Exception as e:
        # ì‹¤íŒ¨ ë¡œê·¸ ì—…ë°ì´íŠ¸
        with hook.get_conn() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    "UPDATE processing_log SET status = 'failed', end_time = %s, error_message = %s WHERE processing_id = %s",
                    (datetime.now(), str(e), processing_id)
                )
        raise
```

#### 2-2. í¬ê´„ì  í…ŒìŠ¤íŠ¸ êµ¬í˜„
```python
import pytest
from unittest.mock import patch, MagicMock

def test_unified_data_sync_success():
    """ì„±ê³µ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
    context = {
        'data_interval_start': datetime(2024, 1, 1),
        'data_interval_end': datetime(2024, 1, 2),
        'dag_run': MagicMock()
    }
    
    with patch('your_module.PostgresHook') as mock_hook:
        mock_conn = MagicMock()
        mock_cursor = MagicMock()
        mock_hook.return_value.get_conn.return_value.__enter__.return_value = mock_conn
        mock_conn.cursor.return_value.__enter__.return_value = mock_cursor
        
        result = unified_data_sync(context)
        
        assert result['status'] == 'success'
        assert mock_cursor.execute.called

def test_unified_data_sync_idempotency():
    """ë©±ë“±ì„± í…ŒìŠ¤íŠ¸"""
    # ë™ì¼í•œ contextë¡œ ë‘ ë²ˆ ì‹¤í–‰
    context = {
        'data_interval_start': datetime(2024, 1, 1),
        'data_interval_end': datetime(2024, 1, 2),
        'dag_run': MagicMock()
    }
    
    # ì²« ë²ˆì§¸ ì‹¤í–‰
    result1 = unified_data_sync(context)
    # ë‘ ë²ˆì§¸ ì‹¤í–‰
    result2 = unified_data_sync(context)
    
    # ê²°ê³¼ê°€ ë™ì¼í•´ì•¼ í•¨
    assert result1 == result2
```

### Phase 3: ì¤‘ê¸° ìµœì í™” (2-4ì£¼)
**ëª©í‘œ**: ì„±ëŠ¥ ìµœì í™” ë° ê³ ê¸‰ ê¸°ëŠ¥

#### 3-1. ì˜¤ë²„ë© ìœˆë„ìš° êµ¬í˜„
```python
@task
def unified_data_sync_with_overlap(context):
    """ì˜¤ë²„ë© ìœˆë„ìš°ë¥¼ ì ìš©í•œ ë°ì´í„° ë™ê¸°í™”"""
    base_start = context['data_interval_start']
    base_end = context['data_interval_end']
    
    # 5ë¶„ ì˜¤ë²„ë© ìœˆë„ìš° ì ìš©
    overlap_minutes = 5
    actual_start = base_start - timedelta(minutes=overlap_minutes)
    actual_end = base_end + timedelta(minutes=overlap_minutes)
    
    logger.info(f"Processing with overlap: {actual_start} to {actual_end}")
    
    # ê¸°ì¡´ ë¡œì§ì— ì˜¤ë²„ë© ìœˆë„ìš° ì ìš©
    return process_with_overlap(actual_start, actual_end, base_start, base_end)
```

#### 3-2. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```python
import time
from airflow.providers.postgres.hooks.postgres import PostgresHook

@task
def unified_data_sync_monitored(context):
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ì´ í¬í•¨ëœ ë°ì´í„° ë™ê¸°í™”"""
    start_time = time.time()
    
    try:
        result = unified_data_sync_core(context)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
        execution_time = time.time() - start_time
        record_performance_metrics({
            'execution_time': execution_time,
            'processed_records': result.get('processed_records', 0),
            'dag_run_id': context['dag_run'].run_id,
            'task_instance': context['task_instance'].task_id
        })
        
        return result
        
    except Exception as e:
        # ì˜¤ë¥˜ ë©”íŠ¸ë¦­ ê¸°ë¡
        record_error_metrics({
            'error_type': type(e).__name__,
            'error_message': str(e),
            'execution_time': time.time() - start_time
        })
        raise
```

## ğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

### 1. ì ì§„ì  ì „í™˜
```python
# ê¸°ì¡´ DAGì™€ ìƒˆ DAG ë³‘ë ¬ ì‹¤í–‰
with DAG('data_sync_legacy', schedule_interval='@hourly') as legacy_dag:
    # ê¸°ì¡´ ë©€í‹°íƒœìŠ¤í¬ êµ¬ì¡°
    pass

with DAG('data_sync_unified', schedule_interval='@hourly') as unified_dag:
    # ìƒˆë¡œìš´ ë‹¨ì¼íƒœìŠ¤í¬ êµ¬ì¡°
    unified_data_sync()
```

### 2. A/B í…ŒìŠ¤íŠ¸
```python
@task
def data_validation_comparison(context):
    """ê¸°ì¡´ ë°©ì‹ê³¼ ìƒˆ ë°©ì‹ì˜ ê²°ê³¼ ë¹„êµ"""
    legacy_result = get_legacy_processing_result(context)
    unified_result = get_unified_processing_result(context)
    
    comparison = compare_results(legacy_result, unified_result)
    
    if comparison['match_percentage'] < 95:
        raise ValueError(f"Results mismatch: {comparison}")
    
    return comparison
```

### 3. ë¡¤ë°± ê³„íš
```python
# ë¬¸ì œ ë°œìƒ ì‹œ ì¦‰ì‹œ ê¸°ì¡´ DAGë¡œ ë¡¤ë°±
def emergency_rollback():
    # ìƒˆ DAG ë¹„í™œì„±í™”
    disable_dag('data_sync_unified')
    # ê¸°ì¡´ DAG ì¬í™œì„±í™”
    enable_dag('data_sync_legacy')
    # ì•Œë¦¼ ë°œì†¡
    send_alert("Rolled back to legacy architecture")
```

## ğŸ” ì„±ê³µ ì§€í‘œ

### ë°ì´í„° í’ˆì§ˆ ì§€í‘œ
- **ë°ì´í„° ëˆ„ë½ë¥ **: 0% ëª©í‘œ
- **ì¤‘ë³µ ì²˜ë¦¬ìœ¨**: 0% ëª©í‘œ
- **ì²˜ë¦¬ ì¼ê´€ì„±**: 100% ë™ì¼ ê²°ê³¼

### ìš´ì˜ íš¨ìœ¨ì„± ì§€í‘œ
- **ì‹¤í–‰ ì‹œê°„**: ê¸°ì¡´ ëŒ€ë¹„ Â±20% ì´ë‚´
- **ì‹¤íŒ¨ìœ¨**: 50% ê°ì†Œ ëª©í‘œ
- **ë³µêµ¬ ì‹œê°„**: 80% ë‹¨ì¶• ëª©í‘œ

### ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
```python
def create_monitoring_dashboard():
    """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
    metrics = {
        'daily_processed_records': get_daily_processed_count(),
        'average_execution_time': get_average_execution_time(),
        'error_rate': get_error_rate(),
        'data_quality_score': calculate_data_quality_score()
    }
    return metrics
```

## ğŸ¯ ìµœì¢… ê¶Œì¥ì‚¬í•­

### ì¦‰ì‹œ ì‹¤í–‰ ì‚¬í•­
1. **ë‹¨ì¼ íƒœìŠ¤í¬ ì•„í‚¤í…ì²˜ë¡œ ì¦‰ì‹œ ì „í™˜** - ë°ì´í„° ë¬´ê²°ì„± ìœ„í—˜ í•´ê²° ìµœìš°ì„ 
2. **data_interval_start/end ì‚¬ìš©** - ì¼ê´€ëœ ì‹œê°„ ìœˆë„ìš° ë³´ì¥
3. **ì‹œê°„ëŒ€ ë³€í™˜ êµ¬í˜„** - Pendulumìœ¼ë¡œ UTC â†’ KST ë³€í™˜ (DBê°€ í•œêµ­ ì‹œê°„ëŒ€ì¸ ê²½ìš°)
4. **UPSERT ì—°ì‚° êµ¬í˜„** - ì¬ì‹¤í–‰ ì•ˆì „ì„± í™•ë³´

### ë‹¨ê³„ë³„ ìš°ì„ ìˆœìœ„
1. **ğŸš¨ ê¸´ê¸‰ (1-2ì¼)**: ê¸°ë³¸ ë‹¨ì¼ íƒœìŠ¤í¬ êµ¬í˜„
2. **âš¡ ë‹¨ê¸° (1ì£¼)**: ë©±ë“±ì„± ë° ëª¨ë‹ˆí„°ë§ ê°•í™”  
3. **ğŸ—ï¸ ì¤‘ê¸° (2-4ì£¼)**: ì„±ëŠ¥ ìµœì í™” ë° ê³ ê¸‰ ê¸°ëŠ¥

### í•µì‹¬ ì„±ê³µ ìš”ì†Œ
- **ì›ìì  íŠ¸ëœì­ì…˜**: ì „ì²´ íŒŒì´í”„ë¼ì¸ì˜ ì¼ê´€ì„± ë³´ì¥
- **í¬ê´„ì  í…ŒìŠ¤íŠ¸**: ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°ì´í„° ì²˜ë¦¬
- **ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜**: ìœ„í—˜ ìµœì†Œí™”ëœ ì „í™˜

## ğŸ“š ì°¸ê³  ìë£Œ

- [Apache Airflow ê³µì‹ ë¬¸ì„œ - Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [TaskFlow API ê°€ì´ë“œ](https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html)  
- [Airflow ì‹œê°„ëŒ€ ì²˜ë¦¬ ê°€ì´ë“œ](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/timezone.html)
- [Pendulum ë¼ì´ë¸ŒëŸ¬ë¦¬ ê³µì‹ ë¬¸ì„œ](https://pendulum.eustace.io/docs/)
- [Dynamic Task Mapping](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html)

---

**ê²°ë¡ **: í˜„ì¬ì˜ ë©€í‹°íƒœìŠ¤í¬ ì•„í‚¤í…ì²˜ëŠ” ê·¼ë³¸ì ì¸ ë°ì´í„° ë¬´ê²°ì„± ìœ„í—˜ì„ ë‚´í¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¨ì¼ í†µí•© íƒœìŠ¤í¬ ì•„í‚¤í…ì²˜ë¡œì˜ ì „í™˜ì€ ì´ëŸ¬í•œ ìœ„í—˜ì„ ì™„ì „íˆ í•´ê²°í•˜ë©´ì„œë„ ìš´ì˜ ë³µì¡ì„±ì„ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆëŠ” ìµœì ì˜ í•´ê²°ì±…ì…ë‹ˆë‹¤.